{
    "model_class": "BidirectionalAttentionFlow",
    "model_serialization_prefix": "/tmp/models/bidaf",
    "encoder": {
        "word": {
          "type": "cnn",
          "ngram_filter_sizes": [5],
          "num_filters": 100
        }
    },
    "seq2seq_encoder": {
        "default": {
            "type": "bi_gru",
            "encoder_params": {
                "output_dim": 100
            },
            "wrapper_params": {}
        }
    },
    // This is not quite the same as Min's paper; we don't have encoder dropout yet.
    "embedding_dropout": 0.2,
    "batch_size": 10,
    "patience": 3,
    "embedding_size": 100,
    "num_epochs": 20,
    "optimizer": {
      "type": "adadelta",
      "lr": 0.5
    },
    "validation_files": ["/efs/data/dlfa/squad/processed/dev.tsv"],
    "train_files": ["/efs/data/dlfa/squad/processed/train.tsv"]
}
